{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of a simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical structure of a feed-forward neural network is as follows:\n",
    "\n",
    "<img src=\"../images/structure-network.png\" width=\"400px\" alt=\"\" align=\"left\">\n",
    "<br clear=\"all\" />\n",
    "\n",
    "The hidden level/layer is used to transform the input layer values into values in a higher-dimensional space, so that we can learn more features from the input. The hidden layer transforms the output as follows:\n",
    "\n",
    "<img src=\"../images/hidden-layer.png\" width=\"400px\" alt=\"\" align=\"left\">\n",
    "<br clear=\"all\" />\n",
    "\n",
    "Note that `w1,w2,...,wn` are the weights given to each of the input variables. If `a` is one of the units in the hidden layer, it will be equal to the following:\n",
    "\n",
    "\\begin{equation}\n",
    "a=f\\left(\\sum_{i=0}^{N} w_{i} x_{i}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The `f` function is the activation function that is used to apply non-linearity on top of the sum-product of the input and their corresponding weight values. Additionally, higher non-linearity can be achieved by having more than one hidden layer.\n",
    "\n",
    "In sum, a neural network is a collection of weights assigned to nodes with layers connecting them. Note that you can have `n` hidden layers, with the term **deep learning** implying multiple hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a neural network\n",
    "Training a neural network basically means calibrating all of the weights in a neural network by repeating two key steps: forward-propagation and back-propagation.\n",
    "\n",
    "In *forward-propagation*, we apply a set of weights to the input data, pass it through the hidden layer, perform the nonlinear activation on the hidden layer output, and then connect the hidden layer to the output layer by multiplying the hidden layer node values with another set of weights. For the first forward-propagation, the values of the weights are initialized randomly.\n",
    "\n",
    "In *back-propagation*, we try to decrease the error by measuring the margin of error of output and then adjust weight accordingly. Neural networks repeat both forward- and back-propagation to predict an output until the weights are calibrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can be architected in multiple ways. Here are some of the possible ways:\n",
    "\n",
    "<img src=\"../images/architecture-network.png\" width=\"600px\" alt=\"\" align=\"left\">\n",
    "<br clear=\"all\" />\n",
    "\n",
    "The box at the bottom is the input, followed by the hidden layer (the middle box), and the box at the top is the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward propagation from scratch in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ready\n",
    "\n",
    "<img src=\"../images/net1.png\" width=\"400px\" alt=\"\" align=\"left\">\n",
    "<br clear=\"all\" />\n",
    "\n",
    "### Calculating the hidden layer unit values\n",
    "We now assign weights to all of the connections. Note that these weights are selected randomly (based on Gaussian distribution) since it is the first time we're forward-propagating. \n",
    "\n",
    "<img src=\"../images/net2.png\" width=\"400px\" alt=\"\" align=\"left\">\n",
    "<br clear=\"all\" />\n",
    "\n",
    "In the next step, we perform the multiplication of the input with weights to calculate the values of hidden units in the hidden layer.\n",
    "\n",
    "<img src=\"../images/net3.png\" width=\"400px\" alt=\"\" align=\"left\">\n",
    "<br clear=\"all\" />\n",
    "\n",
    "### Applying the activation function\n",
    "Now, we will pass the hidden layer values through an activation function so that we attain non-linearity in our output. The different activation functions are as follows:\n",
    "\n",
    "<img src=\"../images/activation-func.png\" width=\"500px\" alt=\"\" align=\"left\">\n",
    "<br clear=\"all\" />\n",
    "\n",
    "For our example, let’s use the *sigmoid* function for activation. The sigmoid function looks like this, graphically:\n",
    "\n",
    "<img src=\"../images/sigmoid.png\" width=\"300px\" alt=\"\" align=\"left\">\n",
    "<br clear=\"all\" />\n",
    "\n",
    "By applying sigmoid activation to the three hidden layer *sums*, we get the following:\n",
    "\n",
    "<img src=\"../images/net4.png\" width=\"400px\" alt=\"\" align=\"left\">\n",
    "<br clear=\"all\" />\n",
    "\n",
    "### Calculating the output layered values\n",
    "We perform the sum product of the hidden layer values and weight values to calculate the output value. For simplicity, we excluded the bias terms that need to be added at each unit of the hidden layer:\n",
    "\n",
    "<img src=\"../images/net5.png\" width=\"400px\" alt=\"\" align=\"left\">\n",
    "<br clear=\"all\" />\n",
    "\n",
    "### Calculating the loss values \n",
    "Typically, when the variable is a continuous one, the loss value is calculated as the squared error, that is, we try to minimize the mean squared error by varying the weight values associated with the neural network:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m}\\left(h(\\theta)\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "In the preceding equation, $y$ is the actual value of output, $h(x)$ is the transformation that we apply on the input $x$ to obtain a predicted value of $y$, and $m$ is the number of rows in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to calculate the squared error loss values across all data points is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def feed_forward(inputs, outputs, weights):\n",
    "     pre_hidden = np.dot(inputs,weights[0])+weights[1]        \n",
    "     hidden = 1/(1+np.exp(-pre_hidden))\n",
    "     out = np.dot(hidden,weights[2])+weights[3]\n",
    "     squared_error = (np.square(out-outputs))\n",
    "     return squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the hidden layer values by performing the matrix multiplication (dot product) of the input and weights. Additionally, we add the bias values in the hidden layer, as follows:\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<tt>pre_hidden = np.dot(inputs,weights[0])+weights[1]</tt>\n",
    "</div>\n",
    "\n",
    "We perform activation on top of the hidden layer values, as follows:\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<tt>hidden = 1/(1+np.exp(-pre_hidden))</tt>\n",
    "</div>\n",
    "\n",
    "We now calculate the output at the hidden layer by multiplying the output of the hidden layer with weights that connect the hidden layer to the output, and then adding the bias term at the output, as follows:\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<tt>out = np.dot(hidden,weights[2])+weights[3]</tt>\n",
    "</div>\n",
    "\n",
    "In the preceding code, `out` is the predicted output and `outputs` is the actual output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we considered the sigmoid activation on top of the hidden layer values in the preceding code, let's examine other activation functions that are commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanh\n",
    "The tanh activation of a value (the hidden layer unit value) is calculated as follows:\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<tt>def tanh(x):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;return (exp(x)-exp(-x))/(exp(x)+exp(-x)) \n",
    "</tt>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear\n",
    "The linear activation of a value is the value itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "Typically, softmax is performed on top of a vector of values. This is generally done to determine the probability of an input belonging to one of the `n` number of the possible output classes in a given scenario. Let's say we are trying to classify an image of a digit into one of the possible 10 classes (numbers from 0 to 9). \n",
    "\n",
    "The softmax activation is used to provide a probability value for each class in the output and is calculated as follows: \n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<tt>def softmax(x):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;return np.exp(x)/np.sum(np.exp(x)) \n",
    "</tt>\n",
    "</div>\n",
    "\n",
    "Apart from the preceding activation functions, the loss functions that are generally used while building a neural network are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean squared error\n",
    "The error is the difference between the actual `y` and predicted `p` values of the output: \n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<tt>def mse(p, y):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;return np.mean(np.square(p - y)) \n",
    "</tt>\n",
    "</div>\n",
    "\n",
    "The mean squared error is typically used when trying to predict a value that is continuous in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical cross-entropy\n",
    "Cross-entropy is a measure of the difference between two different distributions: actual `y` and predicted `p`. It is applied to categorical (discrete) output data:\n",
    "\n",
    "\\begin{equation}\n",
    "-\\left(y \\log_{2} p+(1-y) \\log_{2}(1-p)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "It is implemented as follows:\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<tt>def cat_cross_entropy(p, y):<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;return -np.sum(y*np.log2(p)+(1-y)*np.log2(1-p)) \n",
    "</tt>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation from scratch in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ready\n",
    "In *forward* propagation, we connected the input layer to the hidden layer to the output layer. In *back* propagation, we take the reverse approach.\n",
    "\n",
    "A change in the weight value will have an impact on the final loss value (either increasing or decreasing loss). We'll update the weight in the direction of decreasing loss. By updating the weights by a small amount and measuring the change in error that the update in weights leads to, we are able to do the following:\n",
    "* Determine the direction of the weight update\n",
    "* Determine the magnitude of the weight update\n",
    "\n",
    "**Back propagation** works as follows:\n",
    "* Calculates the overall cost function from the feed-forward process.\n",
    "* Varies all the weights (one at a time) by a small amount. \n",
    "* Calculates the impact of the variation of weight on the cost function.\n",
    "* Depending on whether the change has an increased or decreased the cost (loss) value, it updates the weight value in the direction of loss decrease. And then repeats this step across all the weights we have. \n",
    "\n",
    "If the preceding steps are performed `n` number of times, it essentially results in `n` **epochs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it...\n",
    "In this section, we will build the back-propagation algorithm by hand so that we clearly understand how weights are calculated in a neural network. In this specific case, we will build a simple neural network where there is no hidden layer (thus we are solving a regression equation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize the dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1],[2],[3],[4]]\n",
    "y = [[2],[4],[6],[8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Initialize the weight and bias values randomly (we have only one weight and one bias value as we are trying to identify the optimal values of $a$ and $b$ in the $y = a\\cdot x + b$ equation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [1.5, 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define the feed-forward network and calculate the squared error loss value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T15:01:04.248231Z",
     "start_time": "2019-02-06T15:01:04.244978Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Z9NcQEFFgXLl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def feed_forward(inputs, outputs, weights):\n",
    "    out = np.dot(inputs,weights[0]) + weights[1]\n",
    "    squared_error = (np.square(out - outputs))\n",
    "    return squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Increase each weight and bias value by a very small amount (0.0001) and calculate the squared error loss value one at a time for each of the weight and bias updates. \n",
    "\n",
    "   If the squared error loss value decreases as the weight increases, the weight value should be increased. The magnitude by which the weight value should be increased is proportional to the amount of loss decrease (weigh it down with a factor called *learning rate*).\n",
    "   \n",
    "   In the following code, we create a function which performs the back propagation process to update weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def update_weights(inputs, outputs, weights, epochs):  \n",
    "    for epoch in range(epochs):\n",
    "        org_loss = feed_forward(inputs, outputs, weights)  \n",
    "        wts_tmp = deepcopy(weights)\n",
    "        wts_tmp2 = deepcopy(weights)\n",
    "        for ix, wt in enumerate(weights): \n",
    "            wts_tmp[-(ix+1)] += 0.0001\n",
    "            # print('wts_tmp:', wts_tmp)\n",
    "            loss = feed_forward(inputs, outputs, wts_tmp)\n",
    "            # print('loss', loss)\n",
    "            delta_loss = np.sum(org_loss - loss)/(0.0001*len(inputs))\n",
    "            wts_tmp2[-(ix+1)] += delta_loss*0.01\n",
    "            wts_tmp = deepcopy(weights)\n",
    "\n",
    "        weights = deepcopy(wts_tmp2)\n",
    "    return wts_tmp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `deepcopy` so that the value of the original variable does not change when the variable to which the original variable's values are copied has its values changed.\n",
    "\n",
    "Loop through all the weight values, one at a time, and change them by a small value (0.0001):\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<tt>wts_tmp[-(ix+1)] += 0.0001</tt>\n",
    "</div>\n",
    "\n",
    "Calculate the updated feed-forward loss when the weight is updated by a small amount. Then calculating its impact on loss value is equivalent to performing a derivative with respect to change in weight:\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<tt>loss = feed_forward(inputs, outputs, wts_tmp)<br>\n",
    "delta_loss = np.sum(org_loss-loss)/(0.0001*len(inputs))\n",
    "</tt>  \n",
    "</div>  \n",
    "\n",
    "Update the weights by the change in loss that they are causing. Update the weights slowly by multiplying the change in loss by a very small number (0.01), which is the *learning rate* parameter:\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<tt>wts_tmp2[-(ix+1)] += delta_loss*0.01</tt>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5749925000000138, 0.024998999999971405]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_weights(x,y,w,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_val = []\n",
    "b_val = []\n",
    "for k in range(100):\n",
    "    w_new, b_new = update_weights(x,y,w,(k+1))\n",
    "    w_val.append(w_new)\n",
    "    b_val.append(b_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(w_val)\n",
    "plt.title('Weight value over different epochs when initial weight is 1.5')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('weight value')\n",
    "plt.grid('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "<link href='https://fonts.googleapis.com/css?family=Source+Code+Pro' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width */\n",
       "    width: 750px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    width: 1000px;\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running { \n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256); \n",
       "    border-radius: 0px; \n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 105%;\n",
       "    font-weight: 400;\n",
       "    width:600px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;    \n",
       "    font-size: 40pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 25pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\t\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 17pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 10pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 11pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 4pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"Source Code Pro\";\n",
       "    font-size: 110%;\n",
       "}\n",
       "\n",
       ".alert-box {\n",
       "    padding:10px 10px 10px 36px;\n",
       "    margin:5px;\n",
       "}\n",
       "\n",
       ".success {\n",
       "    color:#666600;\n",
       "    background:rgb(240,242,229);\n",
       "}\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                            extensions: [\"AMSmath.js\"],\n",
       "                            equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                            },\n",
       "                        tex2jax: {\n",
       "                            inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                            displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                            },\n",
       "                        displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                        \"HTML-CSS\": {\n",
       "                            styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                            }\n",
       "                        });\n",
       "    MathJax.Hub.Queue(\n",
       "                      [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
       "                      [\"PreProcess\", MathJax.Hub],\n",
       "                      [\"Reprocess\", MathJax.Hub]\n",
       "                     );\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "css_file = '.././styles/numericalmoocstyle.css'\n",
    "HTML(open(css_file, 'r').read())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neural_network_working_details.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
